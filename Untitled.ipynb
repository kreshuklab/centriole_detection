{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mrcfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(ref_dirs):\n",
    "    gmin = 1e8 \n",
    "    gmax = 0\n",
    "    cu_p = 0\n",
    "    cu_sp = 0\n",
    "    N = 0\n",
    "    for dir_name in ref_dirs:\n",
    "        img_names = [f for f in os.listdir(dir_name) if f.endswith('.mrc')]\n",
    "        N += len(img_names)\n",
    "        for img_name in tqdm(img_names):\n",
    "            with mrcfile.open(os.path.join(dir_name, img_name)) as img:\n",
    "                if len(img.data.shape) > 2:\n",
    "                    continue\n",
    "                gmin = min(gmin, img.data.min())\n",
    "                gmax = max(gmax, img.data.max())\n",
    "                cu_p  += img.data.mean()\n",
    "                apf = np.vectorize(lambda x: x**2)\n",
    "                cu_sp += apf(img.data).mean()\n",
    "    return gmin, gmax, cu_p/N, np.sqrt(cu_sp/N - (cu_p/N)**2)\n",
    "\n",
    "def get_statistics_png(ref_dirs):\n",
    "    gmin = 1e8 \n",
    "    gmax = 0\n",
    "    cu_p = 0\n",
    "    cu_sp = 0\n",
    "    N = 0\n",
    "    for dir_name in ref_dirs:\n",
    "        img_names = [f for f in os.listdir(dir_name) if f.endswith('.png')]\n",
    "        N += len(img_names)\n",
    "        for img_name in tqdm(img_names):\n",
    "            img = cv2.imread(os.path.join(dir_name, img_name))\n",
    "            gmin = min(gmin, img.min())\n",
    "            gmax = max(gmax, img.max())\n",
    "            cu_p  += img.mean()\n",
    "            apf = np.vectorize(lambda x: x**2)\n",
    "            cu_sp += apf(img).mean()\n",
    "    return gmin, gmax, cu_p/N, np.sqrt(cu_sp/N - (cu_p/N)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmin, gmax, gme, gstd = get_statistics(['dataset/cells_containingcentrioles/', 'dataset/cells_withoutcentriole/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmin, gmax, gme, gstd = 8581, 50125, 30801.307306857434, 21336312.23491776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmin, gmax, gme, gstd = get_statistics_png(['dataset/cropped_neg/', 'dataset/cropped_pos/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmin, gmax, gme, gstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread('dataset/cropped_neg/01_273.png')\n",
    "print(im.mean(), im.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_autoscale(img):\n",
    "    return np.uint8((img - gmin) / (gmax - gmin) * 255)\n",
    "\n",
    "def local_autoscale(img):\n",
    "    return np.uint8((img - img.min()) / (img.max() - img.min()) * 255)\n",
    "\n",
    "def normilize(img):\n",
    "    tmp = np.uint8((img - gmin) / np.sqrt(gstd))\n",
    "#     tmp = (img - gmin) / np.sqrt(gstd)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def flood_fill(im_th):\n",
    "    im_floodfill = im_th.copy()\n",
    "    \n",
    "    # Mask used to flood filling.\n",
    "    # Notice the size needs to be 2 pixels than the image.\n",
    "    h, w = im_th.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 255);\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    "\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out = im_th | im_floodfill_inv\n",
    "\n",
    "    # Display images.\n",
    "#     cv2.imshow(\"Thresholded Image\", im_th)\n",
    "#     cv2.imshow(\"Floodfilled Image\", im_floodfill)\n",
    "#     cv2.imshow(\"Inverted Floodfilled Image\", im_floodfill_inv)\n",
    "#     cv2.imshow(\"Foreground\", im_out)\n",
    "#     cv2.waitKey(0)\n",
    "    return im_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def crop_the_cell(img, gauss_ker_crop=21, bin_th=0.9*255*gme/gmax, cl_ker=10, fe_ker=30, se_ker=400, debug=1):\n",
    "    img = global_autoscale(img)\n",
    "    h, w = img.shape\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    \n",
    "    blured = cv2.GaussianBlur(img, (gauss_ker_crop, gauss_ker_crop), 0)\n",
    "    ret, bins = cv2.threshold(blured, bin_th, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    close_ker = np.ones((cl_ker, cl_ker),np.uint8)\n",
    "    bins = cv2.morphologyEx(bins, cv2.MORPH_CLOSE, close_ker)\n",
    "    fer_ker = np.ones((fe_ker, fe_ker),np.uint8)\n",
    "    bins = cv2.morphologyEx(bins, cv2.MORPH_ERODE, fer_ker)\n",
    "    \n",
    "    filled = ndimage.binary_fill_holes(bins).astype(np.uint8) * 255\n",
    "    \n",
    "    filled = cv2.morphologyEx(filled, cv2.MORPH_DILATE, fer_ker)\n",
    "    \n",
    "    ser_ker = np.ones((se_ker, se_ker),np.uint8)\n",
    "    filled = cv2.morphologyEx(filled, cv2.MORPH_ERODE, ser_ker)\n",
    "    \n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(filled, 4, cv2.CV_32S)       \n",
    "    \n",
    "    centr_num = -1\n",
    "    for i in range(len(stats[1:])):\n",
    "        cx, cy, cw, ch, ca = stats[i + 1]\n",
    "        cenx = cx + cw / 2\n",
    "        ceny = cy + ch / 2\n",
    "        if h/2 < cx or h/2 > cx + cw or \\\n",
    "           w/2 < cy or w/2 > cy + ch:\n",
    "            continue\n",
    "        if centr_num != -1:\n",
    "            print('Error: Two centred cells!')\n",
    "            return img\n",
    "        centr_num = i + 1\n",
    "    \n",
    "    if centr_num == -1:\n",
    "        print('Error: Could not find cell in da center')\n",
    "\n",
    "        return img\n",
    "    filtered_labels = (labels == centr_num).astype(np.uint8)\n",
    "    \n",
    "    closed = cv2.morphologyEx(filtered_labels, cv2.MORPH_DILATE, ser_ker)\n",
    "    closed = cv2.morphologyEx(closed, cv2.MORPH_DILATE, fer_ker)\n",
    "    \n",
    "#     if debug:\n",
    "#         yield bins\n",
    "#         yield filled\n",
    "#         yield local_autoscale(labels)\n",
    "#         yield local_autoscale(img * closed)\n",
    "    return img * closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def delete_edge(img, canny_th1=80, canny_th2=100):\n",
    "    img = normilize(img)\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    \n",
    "    edges = cv2.Canny(img, canny_th1, canny_th2)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n",
    "    yield edges\n",
    "    \n",
    "    if lines is not None:\n",
    "        for rho,theta in lines[0]:\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a*rho\n",
    "            y0 = b*rho\n",
    "            x1 = int(x0 + 1000*(-b))\n",
    "            y1 = int(y0 + 1000*(a))\n",
    "            x2 = int(x0 - 1000*(-b))\n",
    "            y2 = int(y0 - 1000*(a))\n",
    "            cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "    yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prep(dir_name, funcs=[], out_edge=300):\n",
    "    img_names = [f for f in os.listdir(dir_name) if f.endswith('.mrc')]\n",
    "    i = 0\n",
    "    while True:\n",
    "        with mrcfile.open(os.path.join(dir_name, img_names[i])) as img:\n",
    "            if len(img.data.shape) == 3:\n",
    "                print('ERROR: ' + dir_name + img_names[i] + ' is not just one slice!')\n",
    "                to_show = np.zeros((out_edge, out_edge, 1), np.uint8)\n",
    "            else:\n",
    "                show_list = []\n",
    "                for func in funcs:\n",
    "                    cto_show = cv2.resize(global_autoscale(img.data), (out_edge, out_edge))\n",
    "                    ret = list(func(img.data))\n",
    "                    for fimg in ret:\n",
    "                        cto_show = np.concatenate((cto_show, cv2.resize(fimg, (out_edge, out_edge))), axis=1)\n",
    "                    show_list.append(cto_show)\n",
    "                if len(show_list) > 1:\n",
    "                    max_width = 0\n",
    "                    for img in show_list:\n",
    "                        max_width = max(max_width, img.shape[1])\n",
    "                    for j in range(len(show_list)):\n",
    "                        if show_list[j].shape[1] < max_width:\n",
    "                            add_img = np.zeros((out_edge, max_width - show_list[j].shape[1]), np.uint8)\n",
    "                            show_list[j] = np.concatenate((show_list[j], add_img), axis=1)\n",
    "                    to_show = np.concatenate(show_list, axis=0)\n",
    "                else:\n",
    "                    to_show = show_list[0]\n",
    "                    \n",
    "            cv2.putText(to_show, img_names[i], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(dir_name, to_show)\n",
    "            cv2.imwrite('debug/' + img_names[i][:-4] + '.png', to_show)\n",
    "            k = cv2.waitKey(0)\n",
    "            if(k == 27):\n",
    "                break\n",
    "            if(k == 3):\n",
    "                if len(img_names) - 1 == i:\n",
    "                    print('Last image achieved')\n",
    "                i = min(i + 1, len(img_names) - 1)\n",
    "            if(k == 2):\n",
    "                if i == 0:\n",
    "                    print('First image achieved')\n",
    "                i = max(i - 1, 0)\n",
    "    cv2.destroyAllWindows()\n",
    "    k = cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prep('dataset/cells_containingcentrioles/', [crop_the_cell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store cropped and normilized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_cropped_from_dir(dir_name, out_dir):\n",
    "    img_names = [f for f in os.listdir(dir_name) if f.endswith('.mrc')]\n",
    "    for img_name in tqdm(img_names):\n",
    "        with mrcfile.open(os.path.join(dir_name, img_name)) as img:\n",
    "            if len(img.data.shape) == 3:\n",
    "                print('ERROR: ' + dir_name + img_name + ' is not just one slice!')\n",
    "                continue\n",
    "            out_img = crop_the_cell(img.data, debug=0)\n",
    "            #out_img = cv2.resize(out_img, (2048, 2048))\n",
    "            out_name = os.path.join(out_dir, img_name[:-4] + '.png')\n",
    "            cv2.imwrite(out_name, out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_cropped_from_dir('dataset/cells_containingcentrioles', 'dataset/cropped_pos')\n",
    "store_cropped_from_dir('dataset/cells_withoutcentriole', 'dataset/cropped_neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy CIFAR dataset on the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "data = unpickle('data/cifar-10-batches-py/data_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[b'data'].reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "\n",
    "imgs_0 = []\n",
    "imgs_1 = []\n",
    "for i in range(10000):\n",
    "    if data[b'labels'][i] == 0 and len(imgs_0) < 100:\n",
    "        imgs_0.append(X[i])\n",
    "    if data[b'labels'][i] == 1 and len(imgs_1) < 150:\n",
    "        imgs_1.append(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs_0):\n",
    "    timg = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (2048, 2048))\n",
    "    cv2.imwrite('dataset/0_cifar_class/' + str(i) + '.png', timg)\n",
    "for i, img in enumerate(imgs_1):\n",
    "    timg = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (2048, 2048))\n",
    "    cv2.imwrite('dataset/1_cifar_class/' + str(i) + '.png', timg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from skimage import io, transform\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentriollesDatasetOn(Dataset):\n",
    "    \"\"\"Centriolles dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, pos_dir='dataset/1_cifar_class/',\n",
    "                       neg_dir='dataset/0_cifar_class/', \n",
    "                all_data=False, train=True, fold=0, out_of=1, transform=None, inp_size=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_sample_dir (string): Path to the directory with all positive samples\n",
    "            neg_sample_dir (string): Path to the directory with all negative samples\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.classes = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        def get_img_names(dir_name):\n",
    "            img_names = [f for f in os.listdir(dir_name) if f.endswith('.png')]\n",
    "            if all_data:\n",
    "                return img_names\n",
    "            if out_of == 1:\n",
    "                delimetr = int(0.6 * len(img_names))\n",
    "            else:\n",
    "                delimetr = int((fold + 1)/out_of * len(img_names))\n",
    "            if train:\n",
    "                img_names = img_names[:delimetr]\n",
    "            else:\n",
    "                img_names = img_names[delimetr:]\n",
    "            return img_names\n",
    "\n",
    "        \n",
    "        ## Positive samples\n",
    "        for img_name in get_img_names(pos_dir):\n",
    "            im = Image.open(os.path.join(pos_dir, img_name))\n",
    "            im.load()\n",
    "            im.thumbnail((inp_size, inp_size), Image.ANTIALIAS)\n",
    "            self.samples.append(im.copy())\n",
    "            self.classes.append(1)\n",
    "            im.close\n",
    "            \n",
    "        ## Negative samples\n",
    "        for img_name in get_img_names(neg_dir):\n",
    "            im = Image.open(os.path.join(neg_dir, img_name))\n",
    "            im.load()\n",
    "            im.thumbnail((inp_size, inp_size), Image.ANTIALIAS)\n",
    "            self.samples.append(im.copy())\n",
    "            self.classes.append(0)\n",
    "            im.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            return self.transform(self.samples[idx]), self.classes[idx]\n",
    "        return self.samples[idx], self.classes[idx]\n",
    "    \n",
    "    def class_balance(self):\n",
    "        return np.sum(self.classes) / len(self.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor(0.4879)\n",
      " Std:  tensor(0.2648)\n",
      "CPU times: user 12.3 s, sys: 2.11 s, total: 14.4 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def detect_mean_std():\n",
    "    all_data = CentriollesDatasetOn(all_data=True, transform=transforms.ToTensor()) \n",
    "    for elem in DataLoader(all_data, batch_size=len(all_data)):\n",
    "        inputs, labels = elem\n",
    "        tmp = torchvision.utils.make_grid(inputs)\n",
    "        gme = tmp.mean()\n",
    "        gstd = tmp.std()\n",
    "    return gme, gstd\n",
    "\n",
    "gme, gstd = detect_mean_std()\n",
    "print(\"Mean: \", gme)\n",
    "print(\" Std: \", gstd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tr = transforms.Compose([transforms.RandomRotation(180),\n",
    "                               transforms.RandomVerticalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((gme, ), (gstd, ))])\n",
    "\n",
    "train_ds = CentriollesDatasetOn(transform=final_tr) \n",
    "test_ds  = CentriollesDatasetOn(transform=final_tr, train=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=3)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=4, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_from_batch(img):\n",
    "    npimg = img.numpy()\n",
    "    tr = np.transpose(npimg, (1, 2, 0))\n",
    "    tr -= tr.min()\n",
    "    tr /= tr.max()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['without', 'centr']\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_dl)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "show_from_batch(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 200, 3)\n",
    "        self.conv2 = nn.Conv2d(200, 150, 3)\n",
    "        self.conv3 = nn.Conv2d(150, 100, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(100 * 7 * 7, 20)\n",
    "        self.fc2 = nn.Linear(20, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(1, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans3 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense4 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans4 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense5 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans5 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense6 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(12288, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.trans4(self.dense4(out))\n",
    "        out = self.trans5(self.dense5(out))\n",
    "        out = self.dense6(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "        print(out.size())\n",
    "        out = out.view(-1, self.num_flat_features(out))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseNet(growthRate=12, depth=20, reduction=0.5, bottleneck=True, nClasses=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 48, 2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [4 x 192], m2: [12288 x 2] at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/TH/generic/THTensorMath.cpp:2070",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-915f4adb7d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-fa17cf0e36e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [4 x 192], m2: [12288 x 2] at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/TH/generic/THTensorMath.cpp:2070"
     ]
    }
   ],
   "source": [
    "train_loss_ar = []\n",
    "test_loss_ar = []\n",
    "\n",
    "best_test_loss = 10e8\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        #show_from_batch(torchvision.utils.make_grid(inputs))\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    for i, data in enumerate(test_dl, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    if(test_loss / len(test_dl) < best_test_loss):\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(net, 'best_weight.pt')\n",
    "    \n",
    "    print('[%d, %3d] train_loss: %.5f test_loss: %.5f' % \n",
    "          (epoch + 1, i + 1, running_loss / len(train_dl), test_loss / len(test_dl)))\n",
    "\n",
    "    train_loss_ar.append(running_loss / len(train_dl))\n",
    "    test_loss_ar.append(test_loss / len(test_dl))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(train_loss_ar, label=\"train\")\n",
    "plt.plot(test_loss_ar, label=\"test\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xticks(range(len(train_loss_ar)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test     : %.2f %%' % (100 * correct / total))\n",
    "print(\"Const ans: %.2f %%\" % (100 * test_ds.class_balance()) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
